{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backend script to gather tweets mentioning impostor syndrome\n",
    "  \n",
    "# By Adriana\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweepy is a package that enables Python to use the Twitter API. Tweepy supports accessing Twitter via OAuth, an authentication method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ckey and csecret in keys.py with your application's key and secret.\n",
    "from keys import *  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating a Twitter developer account, we get two keys: API key and API secret key, those are defined in keys.py\n",
    "as ckey and csecret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is a package that translates tweets called status objects created with tweepy into DataFrames used to manipulate status objects and save them as csv files. Pandas uses numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.AppAuthHandler(ckey, csecret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True,\n",
    "\t\t\t\t   wait_on_rate_limit_notify=True)\n",
    "\n",
    "if (not api):\n",
    "    print (\"Can't Authenticate\")\n",
    "    sys.exit(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authentication takes place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchQuery = '#impostersyndrome OR #impostorsyndrome OR imposter syndrome OR impostor syndrome'  # this is what we're searching for\n",
    "maxTweets = 10000000 # Some arbitrary large number\n",
    "tweetsPerQry = 100  # this is the max the API permits\n",
    "fName = 'tweets.csv' # We'll store the tweets in a text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are searching for hashtags as well as strings related to imposte(o)r syndrome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import csv        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to keep the collection of tweets in teets.csv.\n",
    "The first time we run the search, the file will not exist.\n",
    "\n",
    "In order to avoid reloading all tweets every time, we need to keep track of which was the latest tweet retrieved (sinceId). If the file does not exist, sinceId is set to None. Otherwise we need to search for the most recent tweet ID. \n",
    "\n",
    "When files are updated, information is appended at the end of the file. That means that the most recent tweet is somewhere towards the end of the file, but not quite the last row of the file, since every time a group of tweets is added to tweets.csv, they are listed in decreasing order. \n",
    "\n",
    "tweets.csv = [+....-][+...-]...[+...-]. \n",
    "\n",
    "This code finds the first tweet of the last set of tweets written to tweets.csv.\n",
    "A limit case is when the file has only the result of a single search, in which case the first tweet is the most recent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "exists = os.path.isfile(fName)\n",
    "if exists: \n",
    "    with open(fName, 'rt', encoding='utf-8') as f1:\n",
    "        mycsv = csv.reader(f1)\n",
    "        mycsv = list(mycsv)\n",
    "        row_number = -1    # The end of the csv file has the most recent tweets with \n",
    "                           # the largest set of IDs. We will start searching from the\n",
    "                           # last row up until we find the most recent.\n",
    "        lastRowId = mycsv[row_number][1]\n",
    "        firstId = mycsv[2][1]\n",
    "        if firstId > lastRowId:   # if the firstId is bigger that the last one\n",
    "            sinceId = firstId     # there was only one read of tweets, and the firstId is the largest one.\n",
    "        else: # find the largest/newest ID.\n",
    "            row = row_number - 1\n",
    "            botId = lastRowId\n",
    "            topId = mycsv[row][1]\n",
    "            # we loop up from the end of the file until we find the largest ID\n",
    "            # by falling in the previously read set of tweets.\n",
    "            while  (topId != \"t_id\"):\n",
    "                row = row -1\n",
    "                botId = topId\n",
    "                topId = mycsv[row][1]\n",
    "            sinceId = botId       \n",
    "            f1.close()   \n",
    "else:\n",
    "    sinceId = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the code uses the search api to retrieve 100 tweets at a time; a limit set by Twitter.\n",
    "\n",
    "The new_tweets are in object format, but we want to store them as a csv file with a header for the relevant fields and data in each column, so that every row corresponds to a specific tweet. The following code parses the tweet objects into an internal table called DataFrame. The DataFrame has columns for tweet id, user name, tweet text, location, image, and date, which are the fields relevant to reconstruct the url of a tweet, and preview on the map.\n",
    "\n",
    "The parsed object data is then written to teets.csv with data.to_csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading max 10000000 tweets\n"
     ]
    }
   ],
   "source": [
    "# If results only below a specific ID are, set max_id to that ID.\n",
    "# else default to no upper limit, start from the most recent tweet matching the search query.\n",
    "max_id = -1\n",
    "\n",
    "tweetCount = 0\n",
    "print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "with open(fName, 'a') as f:\n",
    "    while tweetCount < maxTweets:\n",
    "        try:\n",
    "            if (max_id <= 0):\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry)\n",
    "                else:\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                            since_id=sinceId)\n",
    "            else:\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                            max_id=str(max_id - 1))\n",
    "                else:\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                            max_id=str(max_id - 1),\n",
    "                                            since_id=sinceId)\n",
    "            if not new_tweets:\n",
    "                break\n",
    "            data = pd.DataFrame(data=[tweet.id for tweet in new_tweets], columns=['t_id'])\n",
    "            data['s_name'] = np.array([tweet.user.screen_name for tweet in new_tweets])\n",
    "            data['t_text'] = np.array([tweet.text for tweet in new_tweets])\n",
    "            data['u_location'] = np.array([tweet.user.location for tweet in new_tweets])\n",
    "            data['image_url'] = np.array([tweet.user.profile_image_url for tweet in new_tweets])\n",
    "            data['t_date'] = np.array([tweet.created_at for tweet in new_tweets])\n",
    "            data.to_csv(fName, encoding='utf-8-sig',  mode='a')\n",
    "            tweetCount += len(new_tweets)\n",
    "            max_id = new_tweets[-1].id\n",
    "        except tweepy.TweepError as e:\n",
    "            # Just exit if any error\n",
    "            print(\"some error : \" + str(e))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally a message with the total tweets retrieved is printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1906 tweets. Saved to tweets.csv\n"
     ]
    }
   ],
   "source": [
    "print (\"Downloaded {0} tweets. Saved to {1}\".format(tweetCount, fName))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
